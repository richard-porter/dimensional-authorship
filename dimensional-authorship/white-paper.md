---

## Appendix: Epistemic Standards

### Confidence Grading System

Every major claim in this case study carries an implicit confidence level. This appendix makes those levels explicit.

| Grade | Criteria | Example Claims |
|-------|----------|----------------|
| **High** | Observed consistently across 5/5 systems tested. Binary outcome, directly measurable. | Hard binary gates reliably alter model behavior. Self-awareness does not equal self-correction. |
| **Medium** | Observed consistency suggests structural pattern, but untested at scale or across broader populations. | Diagnostic vocabulary changes actual behavior, not just narration. Complementary architecture (high-D + low-D pairing) explains governance effectiveness. |
| **Low** | Observed in single-operator context. May reflect operator skill, specific model versions, or particular collaboration dynamics rather than generalizable principles. | The Five Skills hierarchy (Pattern Recognition â†’ Sovereignty Awareness). The CLEAN check as drift detection tool. |
| **Requires Replication** | Specific study designs needed before confidence can be assigned. | Does the 100-Token Boot reduce unsolicited offers measurably? Does the diagnostic vocabulary work for operators who didn't create the terms? |

### Research Identity Statement

I study how large language models behave under constraint. I design small, controlled experiments to observe escalation patterns, bias effects, and governance limits across different systems. I do not build platforms, advocate policy, or scale frameworks. I run disciplined experiments, extract durable principles, and stop when the signal is clear.

### Academic Search Term Taxonomy

This work connects to established literature across seven categories:

| Category | Search Terms |
|----------|-------------|
| **Behavioral Governance** | supervisory control theory, finite-state automata, deterministic safety layers, behavioral guardrails |
| **Escalation & Bias Patterns** | sycophancy in LLMs, reinforcement learning from human feedback limitations, AI confabulation, hallucination detection |
| **Operator Protection** | human-AI interaction safety, automation bias, over-reliance on AI, cognitive offloading risks |
| **Structural Simplicity** | minimal viable safety, constraint-based design, negative space in system design |
| **Validation Discipline** | epistemic calibration in AI, confidence estimation in LLMs, AI uncertainty quantification |
| **Cross-Model Variance** | multi-model evaluation, cross-platform AI testing, behavioral profiling of LLMs |
| **Therapy/High-Gain Domains** | AI in mental health, chatbot-induced psychosis, AI safety in vulnerable populations |
